# 为什么推荐“调整副本数”而不是“扩容磁盘”？

## 1. 扩容磁盘的代价 (非常高)
您提到的“本机设置增大空间”是指给虚拟机扩容。这在技术上是可行的，但操作**极其繁琐且高风险**，需要以下步骤：
1.  **宿主机操作**: 在 VMware/VirtualBox 中关闭所有虚拟机，调整磁盘大小（例如从 50GB 改为 100GB）。
2.  **Linux 分区**: 开机进入 Linux，使用 `fdisk` 对新增加的空间进行分区。
3.  **LVM 扩容**: 将新分区加入 LVM 卷组，扩展逻辑卷，最后扩展文件系统 (`xfs_growfs`)。
4.  **集群重启**: 整个过程需要停机维护，耗时可能超过 1-2 小时，且稍有不慎可能导致**系统崩溃或数据丢失**。

## 2. 调整副本数的优势 (秒级生效)
Hadoop 默认会将一份数据存 3 份（或 2 份）以防止丢失。
*   **当前现状**: 您的 50GB 磁盘存了约 40GB 数据，如果有 2 个副本，实际占用就是 80GB，直接撑爆。
*   **解决方案**: 执行一条命令，告诉 Hadoop **“这份数据我只存 1 份就好”**。
*   **效果**: HDFS 会自动删除多余的副本，瞬间释放出 **50% (约 40GB)** 的空间！
*   **耗时**: **几秒钟**。
*   **风险**: 在单机/测试环境下，副本数设为 1 是完全标准的做法，没有任何副作用。

## 3. 立即执行命令
请在 `hadoop1` 终端执行：
```bash
hdfs dfs -setrep -R -w 1 /ecop
```
执行完后，再用 `hdfs dfsadmin -report` 看一下，空间绝对回来了。
