# EcoPulse 项目开发时间轨迹 (Project Timeline)

---

**2026/2/11 16:18**
*   **事项**: 数据资产初始化与预核算
*   **内容**: 完成了项目全量数据（共 7 个原始文件）的下载与整理，存放于 [row](file:///e:/a_VibeCoding/EcoPulse/data/row) 目录。为确保数据安全，已将其整理为压缩包并同步备份至百度网盘。
*   **技术实现**: 在 `E:\a_VibeCoding\EcoPulse\scripts\count_data_rows.py` 路径下新建 Python 脚本，利用多进程并行技术完成了各分文件及总数据量的精确统计。

**2026/2/11 16:34**
*   **事项**: 环境隔离体系构建
*   **脚本**: [dev_shell.ps1](file:///e:/a_VibeCoding/EcoPulse/scripts/dev_shell.ps1)
*   **内容**: 创建了项目专属的隔离环境及配套自动化脚本。
*   **规范**: 确立了标准工作流——每次启动 IDE 或开启新终端时，必须首先执行 `.\scripts\dev_shell.ps1` 以确保 `JAVA_HOME` 和虚拟环境正确加载。

**2026/2/11 17:49**
*   **事项**: 核心引擎验证与目录规范
*   **路径**: 建立 `CoreCode` 目录，并编写 [test_spark.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/test_spark.py)。
*   **结果**: 成功运行测试脚本，验证了 Spark 环境的稳定性。目前环境已**完美调通**：代码执行顺畅，组件版本匹配，且运行结束后的系统资源回收机制验证正常。

**2026/2/11 17:56**
*   **事项**: 高性能数据采样（Track A 启动）
*   **脚本**: [sample_data.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/sample_data.py)
*   **内容**: 执行数据采样与格式转换逻辑，将原始 CSV 转换为高性能的 Parquet 格式。
*   **操作**: 在激活环境的终端中运行 `python CoreCode/sample_data.py`。
*   **扩展**: 支持带参数运行以抽取不同月份数据，例如：
    ```bash
    python CoreCode/sample_data.py --input data/row/2019-Nov.csv --output data/dwd/sample_nov_2019 --month 2019-11
    ```
*   **当前进度**: 已完成对 `2019-10` 月份数据的“小范围验证”采样。

**2026/2/11 18:12**
*   **事项**: 数据画像与深度体检
*   **脚本 1**: [eda_analysis.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/eda_analysis.py)（探索性分析）
    *   **目标**: 构建“数据画像”，通过统计行为分布、缺失率及价格分位数，深入了解数据的原始形态与架构。
*   **脚本 2**: [data_quality.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/data_quality.py)（数据质量审计）
    *   **目标**: 执行“深度体检”，精准识别异常值（如价格 ≤ 0）及数据分布倾斜。
*   **逻辑总结**: 确立了 **“采样 ➡ 画像 ➡ 体检”** 的三步走策略，遵循工业级大数据处理规范，从根源上防止“垃圾进，垃圾出 (GIGO)”，为后续机器学习建模奠定高质量数据基础。


第一阶段（数据摸底与环境工程）已圆满完成！

---

## 📅 2026-02-11：第二阶段：ETL 数据清洗与明细层构建

**2026/2/11 18:45**
*   **事项**: DWD 明细层 ETL 开发
*   **脚本**: [etl_dwd_user_behavior.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/etl_dwd_user_behavior.py)
*   **内容**: 将原始采样数据清洗并转换为 DWD 明细层。
*   **技术实现**: 
    1.  **去重**: 成功识别并剔除了约 **3 万条** 重复记录。
    2.  **治理**: 标记了 **68,670 条** 价格 ≤ 0 的异常记录。
    3.  **分区**: 采用按日期 (`dt`) 分区的 Parquet 存储，优化检索效率。
*   **结果**: 成功处理 4241.8 万条数据，标志着项目正式进入结构化数仓阶段。

**2026/2/11 18:51**
*   **事项**: 存储性能基准测试
*   **脚本**: [benchmark_storage.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/benchmark_storage.py)
*   **内容**: 对比原始 CSV 与 DWD Parquet 的查询性能。
*   **测试结果**: 
    1.  **单列聚合**: Parquet (1.21s) 比 CSV (4.04s) 快 **3.3倍**。
    2.  **条件过滤**: Parquet (0.22s) 比 CSV (3.92s) 快 **17.8倍**。
*   **结论**: 量化证明了采用列式存储与分区策略对亿级数据分析的巨大性能提升。






