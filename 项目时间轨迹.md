# EcoPulse 项目开发时间轨迹 (Project Timeline)

---

**2026/2/11 16:18**
*   **事项**: 数据资产初始化与预核算
*   **内容**: 完成了项目全量数据（共 7 个原始文件）的下载与整理，存放于 [row](file:///e:/a_VibeCoding/EcoPulse/data/row) 目录。为确保数据安全，已将其整理为压缩包并同步备份至百度网盘。
*   **技术实现**: 在 `E:\a_VibeCoding\EcoPulse\scripts\count_data_rows.py` 路径下新建 Python 脚本，利用多进程并行技术完成了各分文件及总数据量的精确统计。

**2026/2/11 16:34**
*   **事项**: 环境隔离体系构建
*   **脚本**: [dev_shell.ps1](file:///e:/a_VibeCoding/EcoPulse/scripts/dev_shell.ps1)
*   **内容**: 创建了项目专属的隔离环境及配套自动化脚本。
*   **规范**: 确立了标准工作流——每次启动 IDE 或开启新终端时，必须首先执行 `.\scripts\dev_shell.ps1` 以确保 `JAVA_HOME` 和虚拟环境正确加载。

**2026/2/11 17:49**
*   **事项**: 核心引擎验证与目录规范
*   **路径**: 建立 `CoreCode` 目录，并编写 [test_spark.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/test_spark.py)。
*   **结果**: 成功运行测试脚本，验证了 Spark 环境的稳定性。目前环境已**完美调通**：代码执行顺畅，组件版本匹配，且运行结束后的系统资源回收机制验证正常。

**2026/2/11 17:56**
*   **事项**: 高性能数据采样（Track A 启动）
*   **脚本**: [sample_data.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/sample_data.py)
*   **内容**: 执行数据采样与格式转换逻辑，将原始 CSV 转换为高性能的 Parquet 格式。
*   **操作**: 在激活环境的终端中运行 `python CoreCode/sample_data.py`。
*   **扩展**: 支持带参数运行以抽取不同月份数据，例如：
    ```bash
    python CoreCode/sample_data.py --input data/row/2019-Nov.csv --output data/dwd/sample_nov_2019 --month 2019-11
    ```
*   **当前进度**: 已完成对 `2019-10` 月份数据的“小范围验证”采样。

**2026/2/11 18:12**
*   **事项**: 数据画像与深度体检
*   **脚本 1**: [eda_analysis.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/eda_analysis.py)（探索性分析）
    *   **目标**: 构建“数据画像”，通过统计行为分布、缺失率及价格分位数，深入了解数据的原始形态与架构。
*   **脚本 2**: [data_quality.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/data_quality.py)（数据质量审计）
    *   **目标**: 执行“深度体检”，精准识别异常值（如价格 ≤ 0）及数据分布倾斜。
*   **逻辑总结**: 确立了 **“采样 ➡ 画像 ➡ 体检”** 的三步走策略，遵循工业级大数据处理规范，从根源上防止“垃圾进，垃圾出 (GIGO)”，为后续机器学习建模奠定高质量数据基础。


第一阶段（数据摸底与环境工程）已圆满完成！

---

## 📅 2026-02-11：第二阶段：ETL 数据清洗与明细层构建

**2026/2/11 18:45**
*   **事项**: DWD 明细层 ETL 开发
*   **脚本**: [etl_dwd_user_behavior.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/etl_dwd_user_behavior.py)
*   **内容**: 将原始采样数据清洗并转换为 DWD 明细层。
*   **技术实现**: 
    1.  **去重**: 成功识别并剔除了约 **3 万条** 重复记录。
    2.  **治理**: 标记了 **68,670 条** 价格 ≤ 0 的异常记录。
    3.  **分区**: 采用按日期 (`dt`) 分区的 Parquet 存储，优化检索效率。
*   **结果**: 成功处理 4241.8 万条数据，标志着项目正式进入结构化数仓阶段。

**2026/2/11 18:51**
*   **事项**: 存储性能基准测试
*   **脚本**: [benchmark_storage.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/benchmark_storage.py)
*   **内容**: 对比原始 CSV 与 DWD Parquet 的查询性能。
*   **测试结果**: 
    1.  **单列聚合**: Parquet (1.21s) 比 CSV (4.04s) 快 **3.3倍**。
    2.  **条件过滤**: Parquet (0.22s) 比 CSV (3.92s) 快 **17.8倍**。
*   **结论**: 量化证明了采用列式存储与分区策略对亿级数据分析的巨大性能提升。




第二阶段的三大核心目标（分层、分区、性能）均已达成

---

## 📅 2026-02-11：第三阶段：业务指标计算与特征工程 (3.0-Analysis)

**2026/2/11 19:25**
*   **事项**: RFM 用户分层模型开发
*   **脚本**: [analysis_rfm.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_rfm.py)
*   **内容**: 基于 DWD 层数据计算用户的 Recency, Frequency, Monetary 指标并进行分位数打分。
*   **结果**: 
    1.  **用户画像**: 识别出 **30.2万** 普通用户和 **4.4万** 新用户。
    2.  **数据洞察**: 发现平台用户消费频次呈显著长尾分布（大部分仅购买 1 次），提示后续运营应聚焦于“首单转化”与“低频激活”。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_user_rfm`。

**2026/2/11 19:29**
*   **事项**: 转化漏斗分析开发
*   **脚本**: [analysis_funnel.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_funnel.py)
*   **内容**: 构建 Session/User 维度的 View->Cart->Purchase 漏斗。
*   **结果**:
    1.  **全站转化率**: 6.8% (Session口径)，指标健康。
    2.  **异常发现**: 购买数 > 加购数，验证了“直接下单”功能的强渗透率。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_funnel_stats`。

**2026/2/11 19:35**
*   **事项**: 用户留存分析 (Cohort Analysis)
*   **脚本**: [analysis_retention.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_retention.py)
*   **内容**: 计算次日、3日及7日用户留存率。
*   **结果**:
    1.  **次日留存**: 17.3%，处于行业正常水平。
    2.  **流失特征**: 用户主要在前3天流失，7日后留存率稳定在 11.8%。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_user_retention`。

**2026/2/11 19:41**
*   **事项**: ADS 层数据校验
*   **脚本**: [verify_ads.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/verify_ads.py)
*   **内容**: 对漏斗、留存、RFM 三大分析结果进行交叉验证。
*   **结果**:
    1.  **一致性**: 漏斗购买人数 (347,118) 与 RFM 总人数完美匹配。
    2.  **合理性**: 留存率 Day 0 恒为 100%，无逻辑错误。
    3.  **状态**: **PASS**。


3.0-Analysis (业务指标计算与特征工程) 已圆满完成。 我们成功实现了：

- ✅ RFM 用户分层 : 识别高价值与流失用户。
- ✅ 转化漏斗 : 洞察 View -> Buy 的转化效率。
- ✅ 留存分析 : 量化用户生命周期粘性。
- ✅ 数据校验 : 确保了所有分析结果的逻辑闭环。



