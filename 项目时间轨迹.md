# EcoPulse 项目开发时间轨迹 (Project Timeline)

---

**2026/2/11 16:18**
*   **事项**: 数据资产初始化与预核算
*   **内容**: 完成了项目全量数据（共 7 个原始文件）的下载与整理，存放于 [row](file:///e:/a_VibeCoding/EcoPulse/data/row) 目录。为确保数据安全，已将其整理为压缩包并同步备份至百度网盘。
*   **技术实现**: 在 `E:\a_VibeCoding\EcoPulse\scripts\count_data_rows.py` 路径下新建 Python 脚本，利用多进程并行技术完成了各分文件及总数据量的精确统计。

**2026/2/11 16:34**
*   **事项**: 环境隔离体系构建
*   **脚本**: [dev_shell.ps1](file:///e:/a_VibeCoding/EcoPulse/scripts/dev_shell.ps1)
*   **内容**: 创建了项目专属的隔离环境及配套自动化脚本。
*   **规范**: 确立了标准工作流——每次启动 IDE 或开启新终端时，必须首先执行 `.\scripts\dev_shell.ps1` 以确保 `JAVA_HOME` 和虚拟环境正确加载。

**2026/2/11 17:49**
*   **事项**: 核心引擎验证与目录规范
*   **路径**: 建立 `CoreCode` 目录，并编写 [test_spark.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/test_spark.py)。
*   **结果**: 成功运行测试脚本，验证了 Spark 环境的稳定性。目前环境已**完美调通**：代码执行顺畅，组件版本匹配，且运行结束后的系统资源回收机制验证正常。

**2026/2/11 17:56**
*   **事项**: 高性能数据采样（Track A 启动）
*   **脚本**: [sample_data.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/sample_data.py)
*   **内容**: 执行数据采样与格式转换逻辑，将原始 CSV 转换为高性能的 Parquet 格式。
*   **操作**: 在激活环境的终端中运行 `python CoreCode/sample_data.py`。
*   **扩展**: 支持带参数运行以抽取不同月份数据，例如：
    ```bash
    python CoreCode/sample_data.py --input data/row/2019-Nov.csv --output data/dwd/sample_nov_2019 --month 2019-11
    ```
*   **当前进度**: 已完成对 `2019-10` 月份数据的“小范围验证”采样。

**2026/2/11 18:12**
*   **事项**: 数据画像与深度体检
*   **脚本 1**: [eda_analysis.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/eda_analysis.py)（探索性分析）
    *   **目标**: 构建“数据画像”，通过统计行为分布、缺失率及价格分位数，深入了解数据的原始形态与架构。
*   **脚本 2**: [data_quality.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/data_quality.py)（数据质量审计）
    *   **目标**: 执行“深度体检”，精准识别异常值（如价格 ≤ 0）及数据分布倾斜。
*   **逻辑总结**: 确立了 **“采样 ➡ 画像 ➡ 体检”** 的三步走策略，遵循工业级大数据处理规范，从根源上防止“垃圾进，垃圾出 (GIGO)”，为后续机器学习建模奠定高质量数据基础。


第一阶段（数据摸底与环境工程）已圆满完成！





