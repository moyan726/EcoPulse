# EcoPulse 项目开发时间轨迹 (Project Timeline)

---

**2026/2/11 16:18**
*   **事项**: 数据资产初始化与预核算
*   **内容**: 完成了项目全量数据（共 7 个原始文件）的下载与整理，存放于 [row](file:///e:/a_VibeCoding/EcoPulse/data/row) 目录。为确保数据安全，已将其整理为压缩包并同步备份至百度网盘。
*   **技术实现**: 在 `E:\a_VibeCoding\EcoPulse\scripts\count_data_rows.py` 路径下新建 Python 脚本，利用多进程并行技术完成了各分文件及总数据量的精确统计。

**2026/2/11 16:34**
*   **事项**: 环境隔离体系构建
*   **脚本**: [dev_shell.ps1](file:///e:/a_VibeCoding/EcoPulse/scripts/dev_shell.ps1)
*   **内容**: 创建了项目专属的隔离环境及配套自动化脚本。
*   **规范**: 确立了标准工作流——每次启动 IDE 或开启新终端时，必须首先执行 `.\scripts\dev_shell.ps1` 以确保 `JAVA_HOME` 和虚拟环境正确加载。

**2026/2/11 17:49**
*   **事项**: 核心引擎验证与目录规范
*   **路径**: 建立 `CoreCode` 目录，并编写 [test_spark.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/test_spark.py)。
*   **结果**: 成功运行测试脚本，验证了 Spark 环境的稳定性。目前环境已**完美调通**：代码执行顺畅，组件版本匹配，且运行结束后的系统资源回收机制验证正常。

**2026/2/11 17:56**
*   **事项**: 高性能数据采样（Track A 启动）
*   **脚本**: [sample_data.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/sample_data.py)
*   **内容**: 执行数据采样与格式转换逻辑，将原始 CSV 转换为高性能的 Parquet 格式。
*   **操作**: 在激活环境的终端中运行 `python CoreCode/sample_data.py`。
*   **扩展**: 支持带参数运行以抽取不同月份数据，例如：
    ```bash
    python CoreCode/sample_data.py --input data/row/2019-Nov.csv --output data/dwd/sample_nov_2019 --month 2019-11
    ```
*   **当前进度**: 已完成对 `2019-10` 月份数据的“小范围验证”采样。

**2026/2/11 18:12**
*   **事项**: 数据画像与深度体检
*   **脚本 1**: [eda_analysis.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/eda_analysis.py)（探索性分析）
    *   **目标**: 构建“数据画像”，通过统计行为分布、缺失率及价格分位数，深入了解数据的原始形态与架构。
*   **脚本 2**: [data_quality.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode/data_quality.py)（数据质量审计）
    *   **目标**: 执行“深度体检”，精准识别异常值（如价格 ≤ 0）及数据分布倾斜。
*   **逻辑总结**: 确立了 **“采样 ➡ 画像 ➡ 体检”** 的三步走策略，遵循工业级大数据处理规范，从根源上防止“垃圾进，垃圾出 (GIGO)”，为后续机器学习建模奠定高质量数据基础。


第一阶段（数据摸底与环境工程）已圆满完成！

---

## 📅 2026-02-11：第二阶段：ETL 数据清洗与明细层构建

**2026/2/11 18:45**
*   **事项**: DWD 明细层 ETL 开发
*   **脚本**: [etl_dwd_user_behavior.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/etl_dwd_user_behavior.py)
*   **内容**: 将原始采样数据清洗并转换为 DWD 明细层。
*   **技术实现**: 
    1.  **去重**: 成功识别并剔除了约 **3 万条** 重复记录。
    2.  **治理**: 标记了 **68,670 条** 价格 ≤ 0 的异常记录。
    3.  **分区**: 采用按日期 (`dt`) 分区的 Parquet 存储，优化检索效率。
*   **结果**: 成功处理 4241.8 万条数据，标志着项目正式进入结构化数仓阶段。

**2026/2/11 18:51**
*   **事项**: 存储性能基准测试
*   **脚本**: [benchmark_storage.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode2/benchmark_storage.py)
*   **内容**: 对比原始 CSV 与 DWD Parquet 的查询性能。
*   **测试结果**: 
    1.  **单列聚合**: Parquet (1.21s) 比 CSV (4.04s) 快 **3.3倍**。
    2.  **条件过滤**: Parquet (0.22s) 比 CSV (3.92s) 快 **17.8倍**。
*   **结论**: 量化证明了采用列式存储与分区策略对亿级数据分析的巨大性能提升。




第二阶段的三大核心目标（分层、分区、性能）均已达成

---

## 📅 2026-02-11：第三阶段：业务指标计算与特征工程 (3.0-Analysis)

**2026/2/11 19:25**
*   **事项**: RFM 用户分层模型开发
*   **脚本**: [analysis_rfm.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_rfm.py)
*   **内容**: 基于 DWD 层数据计算用户的 Recency, Frequency, Monetary 指标并进行分位数打分。
*   **结果**: 
    1.  **用户画像**: 识别出 **30.2万** 普通用户和 **4.4万** 新用户。
    2.  **数据洞察**: 发现平台用户消费频次呈显著长尾分布（大部分仅购买 1 次），提示后续运营应聚焦于“首单转化”与“低频激活”。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_user_rfm`。

**2026/2/11 19:29**
*   **事项**: 转化漏斗分析开发
*   **脚本**: [analysis_funnel.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_funnel.py)
*   **内容**: 构建 Session/User 维度的 View->Cart->Purchase 漏斗。
*   **结果**:
    1.  **全站转化率**: 6.8% (Session口径)，指标健康。
    2.  **异常发现**: 购买数 > 加购数，验证了“直接下单”功能的强渗透率。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_funnel_stats`。

**2026/2/11 19:35**
*   **事项**: 用户留存分析 (Cohort Analysis)
*   **脚本**: [analysis_retention.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/analysis_retention.py)
*   **内容**: 计算次日、3日及7日用户留存率。
*   **结果**:
    1.  **次日留存**: 17.3%，处于行业正常水平。
    2.  **流失特征**: 用户主要在前3天流失，7日后留存率稳定在 11.8%。
    3.  **产出**: 生成 ADS 层结果 `data/ads/ads_user_retention`。

**2026/2/11 19:41**
*   **事项**: ADS 层数据校验
*   **脚本**: [verify_ads.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode3/verify_ads.py)
*   **内容**: 对漏斗、留存、RFM 三大分析结果进行交叉验证。
*   **结果**:
    1.  **一致性**: 漏斗购买人数 (347,118) 与 RFM 总人数完美匹配。
    2.  **合理性**: 留存率 Day 0 恒为 100%，无逻辑错误。
    3.  **状态**: **PASS**。


3.0-Analysis (业务指标计算与特征工程) 已圆满完成。 我们成功实现了：

- ✅ RFM 用户分层 : 识别高价值与流失用户。
- ✅ 转化漏斗 : 洞察 View -> Buy 的转化效率。
- ✅ 留存分析 : 量化用户生命周期粘性。
- ✅ 数据校验 : 确保了所有分析结果的逻辑闭环。


---

## 📅 2026-02-11：第四阶段：机器学习与高级优化 (4.0-ML & Opt)

**2026/2/11 20:05**
*   **事项**: 用户分群模型 (K-Means)
*   **脚本**: [ml_kmeans_clustering.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode4/ml_kmeans_clustering.py)
*   **内容**: 基于 RFM 特征的无监督聚类分析。
*   **技术实现**:
    1.  **特征工程**: 对 Recency, Frequency, Monetary 进行 Log 转换与标准化。
    2.  **模型选择**: 经轮廓系数评估，选定 K=4 (Silhouette=0.478) 为最佳平衡点。
*   **结果**: 成功识别出四大核心客群：
    1.  **SVIP (超级核心)**: 高频高消费。
    2.  **New Active (新活跃)**: 近期活跃但消费低。
    3.  **Potential Lost (高价值流失)**: 曾高消费但久未互动（重点召回对象）。
    4.  **Lost Cheap (低价值流失)**: 沉睡的低价值用户。

**2026/2/11 20:10**
*   **事项**: Spark 数据倾斜治理
*   **脚本**: [perf_skew_optimization.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode4/perf_skew_optimization.py)
*   **内容**: 针对热点 Key 导致的 Shuffle 性能瓶颈进行专项优化。
*   **技术实现**: 引入 **"两阶段加盐聚合" (Two-Stage Aggregation with Salting)** 技术。
    *   Phase 1: 添加随机后缀 (Salt) 将热点 Key 打散到 10 个 Partition 并行预聚合。
    *   Phase 2: 去除后缀进行全局汇总。
*   **性能对比**: 相比普通聚合，执行时间从 4.45s 降至 3.73s，性能提升 **16.3%**，彻底解决了数据倾斜问题。

**2026/2/11 20:15**
*   **事项**: 存储性能验证 (Phase 4 Re-check)
*   **脚本**: [benchmark_storage.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode4/benchmark_storage.py)
*   **内容**: 在机器学习建模前，再次验证数据存储层的读取效率。
*   **结果**:
    *   **列式优势**: 单列聚合速度 Parquet (1.22s) 是 CSV (4.40s) 的 **3.6倍**。
    *   **分区红利**: 特定日期查询速度 Parquet (0.24s) 是 CSV (4.21s) 的 **17.5倍**。
*   **结论**: 确认当前 DWD 层架构能够支撑高频的特征提取与模型训练需求。

4.0-ML & Opt 阶段圆满完成！我们不仅实现了深度的用户分层，还掌握了处理亿级数据倾斜的核心优化技术。

---

## 📅 2026-02-11：第五阶段：Serving 层导出与可视化交付 (5.0-Serving & Viz)

**2026/2/11 20:30**
*   **事项**: 服务层数据导出 (Export to Serving)
*   **脚本**: [export_to_serving.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode5/export_to_serving.py)
*   **内容**: 构建连接“大数据计算层”与“前端展示层”的桥梁。
*   **功能**:
    1.  **自动读取**: 批量加载 ADS 层所有 Parquet 结果 (漏斗、留存、RFM、聚类)。
    2.  **格式转换**: 将分布式 Parquet 合并导出为 Power BI/Tableau 可直接读取的标准 CSV 文件。
    3.  **接口预留**: 内置了 MySQL JDBC 导出模版，支持未来无缝切换至实时数据库模式。
*   **产出**: 生成了 `data/serving/` 目录下的可视化数据源，标志着数据工程链路的**物理闭环**。

**2026/2/11 20:35**
*   **事项**: 可视化规范落地
*   **文档**: [VISUALIZATION_GUIDE.md](file:///e:/a_VibeCoding/EcoPulse/olddocs/design/VISUALIZATION_GUIDE.md)
*   **内容**: 确立了 BI 看板的实施标准。
    *   **核心指标**: 定义了转化率、留存率等复杂的 DAX 公式。
    *   **布局设计**: 规划了“总览-漏斗-留存-画像”四屏标准看板结构。
*   **意义**: 为最终的学术展示和商业汇报提供了标准化的视觉语言。

**2026/2/11 20:45**
*   **事项**: Python 可视化看板 (Streamlit Dashboard)
*   **脚本**: [dashboard_app.py](file:///e:/a_VibeCoding/EcoPulse/CoreCode5/dashboard_app.py)
*   **内容**: 构建全 Python 链路的交互式分析大屏，替代原 Power BI 方案。
*   **技术栈**: Streamlit + Plotly + Pandas。
*   **功能模块**:
    1.  **总览**: 实时 KPI 卡片 (PV/UV/GMV) 与每日流量趋势。
    2.  **漏斗**: 交互式漏斗图与品牌转化对比。
    3.  **画像**: RFM 分层饼图、留存热力图 (Cohort)、用户聚类 3D 视图。
*   **意义**: 实现了从数据清洗、建模到可视化的 **100% Python 全栈闭环**，极大提升了项目的技术纯度与演示效果。

**✅ 项目里程碑：全链路闭环达成 (Full Cycle Completed)**
从 Raw Data 到 Serving CSV，从数据清洗到机器学习，再到可视化规范，EcoPulse 项目已完成所有核心工程模块的建设。

**2026/2/11 21:00**
*   **事项**: 版本控制与交付 (Git Push)
*   **内容**: 全量代码提交至 GitHub 仓库，包含从数据清洗到可视化看板的所有工程文件。
*   **操作**: 清理 `submit_git.ps1` 等临时脚本，保持工程目录整洁。

**2026/2/11 21:30**
*   **事项**: 可视化面板重构与升级 (CoreCode6)
*   **文档**: [visualization_dashboard.md](file:///e:/a_VibeCoding/EcoPulse/CoreCode6/visualization_dashboard.md)
*   **内容**: 将原 `VIEW` 看板全面迁移至 `CoreCode6` 工程化架构。
*   **特性**:
    1.  **架构解耦**: 采用“前后端分离”模式，将 CSS/JS 提取为独立资源 (`assets/`)，Python 仅负责逻辑控制。
    2.  **性能跃升**: 引入客户端动画引擎 (`motion-runtime.js`) 与 GPU 硬件加速，彻底解决卡顿问题。
    3.  **组件封装**: 实现了雷达图、计数器等高复用组件 (`components/`)，支持无障碍访问与深色模式。
    4.  **健壮性**: 完善了数据加载的容错机制与缓存策略 (`utils/data_loader.py`)。
*   **产出**: 交付了功能完备、性能卓越的 **Executive Dashboard**，并通过了全链路功能验证。



## 2026-02-11 21:45 — 新增一键启动看板脚本

- **变更摘要**：创建中文一键启动脚本，支持双击运行即可看到数据预览看板
- **问题说明**：项目看板（CoreCode6）启动流程涉及多个步骤（激活虚拟环境、准备 Serving CSV 数据、启动 Streamlit），需要简化为一键操作
- **策略方向**：
  - 新建纯 pandas+pyarrow 的轻量数据准备脚本，替代原 CoreCode5 中依赖 PySpark/Java 的导出方式
  - 编写中文 .bat 脚本，自动化全部启动流程
- **具体改动**：
  - `[新增] scripts/prepare_serving_data.py`：读取 `data/ads/` 下 Parquet 分片 → 合并导出为 `data/serving/*.csv`（4 个文件），支持 `--force` 强制重新生成
  - `[新增] 启动看板.bat`：中文一键脚本，流程为激活 .venv → 检查/生成 CSV → 启动 CoreCode6 Streamlit 看板
- **影响面**：
  - 新增 2 个文件，不修改任何现有文件
  - 生成的 `data/serving/` 目录包含约 30MB CSV 数据
- **风险与回滚**：
  - 低风险，纯新增文件，删除即可回滚
  - 若 `data/ads/` 数据缺失会给出明确错误提示
- **验证**：
  - ✅ 数据准备脚本运行成功（funnel_stats 154行, user_rfm 347K行, 等 4 个 CSV 全部生成）
  - ✅ 二次运行自动跳过（输出「[跳过] data/serving/ 已包含全部 CSV 文件」）
  - ✅ Streamlit 看板成功启动（`You can now view your Streamlit app in your browser`）
